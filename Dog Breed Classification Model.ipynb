{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Dogs - A Classfication problem\n",
    "\n",
    "Classification is a fundamental task in machine learning, and the Stanford Dogs Dataset provides a valuable resource for training and evaluating classification models. The dataset consists of images of various dog breeds, each labeled with the corresponding breed.\n",
    "\n",
    "By leveraging this dataset, we can develop a classification model that can accurately identify the breed of a given dog image. This can have practical applications in areas such as pet identification, animal welfare, and breed-specific research.\n",
    "\n",
    "To build a classification model using the Stanford Dogs Dataset, we can employ various machine learning techniques, such as convolutional neural networks (CNNs). CNNs are particularly effective for image classification tasks, as they can automatically learn relevant features from the input images.\n",
    "\n",
    "By training a CNN on the Stanford Dogs Dataset, we can teach the model to recognize distinctive patterns and characteristics of different dog breeds. Once trained, the model can be used to classify new dog images, providing predictions about the breed with a certain level of confidence.\n",
    "\n",
    "Evaluation of the classification model can be done using metrics such as accuracy, precision, recall, and F1 score. These metrics help assess the model's performance and determine its effectiveness in correctly classifying dog breeds.\n",
    "\n",
    "Overall, the Stanford Dogs Dataset offers a valuable opportunity to explore and develop classification models for dog breed identification. By leveraging this dataset and employing appropriate machine learning techniques, we can contribute to the field of computer vision and enhance our understanding of dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 - Preprocessing ‚öôÔ∏è\n",
    "\n",
    "The dataset is split into two parts - Images and Annotations. \n",
    "\n",
    "The **Images** are pictures of the 120 different dog breeds present in the dataset. \n",
    "The **Annotations** are `.xml`-files, which contains information about where the dog is located in the different pictures and what breed it is.\n",
    "\n",
    "So first of all we need to load all of these informations into Python, so they can be used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16508 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# Define paths\n",
    "images_dir = 'images'\n",
    "\n",
    "# Create the ImageDataGenerator data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2, # 20% of the data will be used for validation\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    ")\n",
    "\n",
    "# Load all images to be used for the training set.\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    images_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Load all images to be used for the validation set.\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    images_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Compiling the model üîß\n",
    "\n",
    "The next step in the process is to compile the model itself. But before that we have define what **Loss function**, **Optimizer** and **Metrics** we are going to be using on this model.\n",
    "\n",
    "For the **Loss function** We have a few different options:\n",
    "\n",
    "(*Name a few different loss functions that would make sense to use for this project.*)\n",
    "\n",
    "For the **Optizimers** we also have a few different options:\n",
    "- *Adam*, *SGD*, *RMSProp* etc.\n",
    "\n",
    "For the **Metrcis** we also have a few different options:\n",
    "- *Accuarcy*, *PRecision*, *Recall*, *F1 score* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# ? Load pre-trained model, if available\n",
    "if os.path.exists('model.h5'):\n",
    "    model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# ? Otherwise, we need to create a new instance of the model.\n",
    "else:\n",
    "    # Load the ResNet50 model, pre-trained on ImageNet\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Add custom layers on top of the base model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Unfreeze the last few layers of the base model\n",
    "    for layer in base_model.layers[-10:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Train the model üß†\n",
    "\n",
    "The next step in the process is to train the now compiled model on our data. Here we also have a little exploratory work in figuring out:\n",
    "- What *batch size* should we use?\n",
    "- What *number of epochs* should we use?\n",
    "- Is the model *overfitting* or *underfitting*?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 39s 311ms/step - loss: 10.5811 - accuracy: 0.7078 - val_loss: 10.6033 - val_accuracy: 0.7212\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 10.5031 - accuracy: 0.7328 - val_loss: 10.6035 - val_accuracy: 0.6862\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 10.5157 - accuracy: 0.7056 - val_loss: 10.5632 - val_accuracy: 0.6825\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 10.4613 - accuracy: 0.7141 - val_loss: 10.3532 - val_accuracy: 0.7600\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 10.4053 - accuracy: 0.7184 - val_loss: 10.3828 - val_accuracy: 0.7255\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 10.3486 - accuracy: 0.7275 - val_loss: 10.3705 - val_accuracy: 0.7150\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 10.3175 - accuracy: 0.7194 - val_loss: 10.3688 - val_accuracy: 0.6988\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 10.2617 - accuracy: 0.7241 - val_loss: 10.3230 - val_accuracy: 0.6888\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 10.2531 - accuracy: 0.7100 - val_loss: 10.1622 - val_accuracy: 0.7450\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 10.1784 - accuracy: 0.7341 - val_loss: 10.1689 - val_accuracy: 0.7216\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 10.1247 - accuracy: 0.7350 - val_loss: 10.1463 - val_accuracy: 0.7262\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 10.0656 - accuracy: 0.7481 - val_loss: 10.1802 - val_accuracy: 0.7038\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 10.0691 - accuracy: 0.7387 - val_loss: 10.0528 - val_accuracy: 0.7525\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 9.9988 - accuracy: 0.7344 - val_loss: 10.1200 - val_accuracy: 0.6950\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 9.9873 - accuracy: 0.7247 - val_loss: 10.0119 - val_accuracy: 0.7384\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 9.9542 - accuracy: 0.7284 - val_loss: 10.0272 - val_accuracy: 0.7188\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 9.8504 - accuracy: 0.7600 - val_loss: 10.0034 - val_accuracy: 0.7075\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 9.8367 - accuracy: 0.7553 - val_loss: 9.9367 - val_accuracy: 0.7212\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 9.8058 - accuracy: 0.7409 - val_loss: 9.8313 - val_accuracy: 0.7139\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 9.7654 - accuracy: 0.7478 - val_loss: 9.7973 - val_accuracy: 0.7275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# - Function the limit the number of batches per epoch for faster iterations.\n",
    "def limit_batches(generator, max_batches):\n",
    "    while True:\n",
    "        for i, (x_batch, y_batch) in enumerate(generator):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "# * Current limits:\n",
    "max_train_batches = 100 # It's a good starting point, but needs to be adjusted for better results.\n",
    "max_validation_batches = 25 # It's a good starting point.\n",
    "\n",
    "# ? Callbacks and their usage\n",
    "\n",
    "# 1. Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1)\n",
    "# 2. Stop training when a monitored quantity has stopped improving.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "# 3. Save the model after every epoch.\n",
    "model_checkpoint = ModelCheckpoint('model.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# ! 1st round of training\n",
    "history = model.fit(\n",
    "    limit_batches(train_generator, max_train_batches),\n",
    "    validation_data=limit_batches(validation_generator, max_validation_batches),\n",
    "    epochs=20, # Use a small number of epochs to speed up the process (10 epochs = 5 mins on GPU - With validation accuracy of 0.18 after 10 epochs)\n",
    "    steps_per_epoch=max_train_batches,\n",
    "    validation_steps=max_validation_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 29s 229ms/step - loss: 9.8302 - accuracy: 0.7205\n",
      "Validation accuracy: 72.05%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // validation_generator.batch_size)\n",
    "print(f'Validation accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher plan!\n",
    "\n",
    "1. **Choose the model architecture suitable for our problem** ü§î\n",
    "    - Convolutional Neural Network (CNN - Good with Image data)\n",
    "    - Recurrent Neural Network (RNN - Good with sequence data)\n",
    "    - Another type??\n",
    "\n",
    "2. **Compile our model** üîß\n",
    "    - What *Loss function* should we use? - Cross-entropy is used for classification?\n",
    "    - What *Optimizer* should we use? Adam, SGD, RMSProp etc.\n",
    "    - What *Metrics* should we use? Accuracy, precision, recall, f1 score etc.\n",
    "\n",
    "3. **Train the model** ‚öôÔ∏è\n",
    "    - What *batch size* should we use?\n",
    "    - What *number of epochs* should we use?\n",
    "    - Is the model *overfitting* or *underfitting*?\n",
    "\n",
    "4. **Evalute the model** üìä\n",
    "    - Is the model performing as we would like? Based upon our selected metrics to be unbiased üòâ\n",
    "\n",
    "5. **Tune Hyperparameter (Optional) - To improve performance** üìà\n",
    "    - Use grid search or another thing similar to find the best hyperparameters\n",
    "    - Adjust model layers, units, learning rate etc.\n",
    "\n",
    "6. **Save the Model (Optional) - But would be smart** üß†\n",
    "    - This can be done, so we don't have to run all the code later to get the model up and running!\n",
    "\n",
    "7. **Use the Model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
