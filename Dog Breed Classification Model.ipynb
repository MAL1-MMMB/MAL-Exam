{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Dogs - A Classfication problem\n",
    "\n",
    "Classification is a fundamental task in machine learning, and the Stanford Dogs Dataset provides a valuable resource for training and evaluating classification models. The dataset consists of images of various dog breeds, each labeled with the corresponding breed.\n",
    "\n",
    "By leveraging this dataset, we can develop a classification model that can accurately identify the breed of a given dog image. This can have practical applications in areas such as pet identification, animal welfare, and breed-specific research.\n",
    "\n",
    "To build a classification model using the Stanford Dogs Dataset, we can employ various machine learning techniques, such as convolutional neural networks (CNNs). CNNs are particularly effective for image classification tasks, as they can automatically learn relevant features from the input images.\n",
    "\n",
    "By training a CNN on the Stanford Dogs Dataset, we can teach the model to recognize distinctive patterns and characteristics of different dog breeds. Once trained, the model can be used to classify new dog images, providing predictions about the breed with a certain level of confidence.\n",
    "\n",
    "Evaluation of the classification model can be done using metrics such as accuracy, precision, recall, and F1 score. These metrics help assess the model's performance and determine its effectiveness in correctly classifying dog breeds.\n",
    "\n",
    "Overall, the Stanford Dogs Dataset offers a valuable opportunity to explore and develop classification models for dog breed identification. By leveraging this dataset and employing appropriate machine learning techniques, we can contribute to the field of computer vision and enhance our understanding of dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 - Preprocessing âš™ï¸\n",
    "\n",
    "The dataset is split into two parts - Images and Annotations. \n",
    "\n",
    "The **Images** are pictures of the 120 different dog breeds present in the dataset. \n",
    "The **Annotations** are `.xml`-files, which contains information about where the dog is located in the different pictures and what breed it is.\n",
    "\n",
    "So first of all we need to load all of these informations into Python, so they can be used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (224, 224, 3) Label: 0\n",
      "Image shape: (224, 224, 3) Label: 0\n",
      "Image shape: (224, 224, 3) Label: 0\n",
      "Image shape: (224, 224, 3) Label: 0\n",
      "Image shape: (224, 224, 3) Label: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "# - Parses the annotation XML file to extract the bounding box coordinates\n",
    "def parse_xml(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # * Extract the bounding box coordinates\n",
    "    bndbox = root.find('.//bndbox')\n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "# - Loads the image and crops it based on the bounding box coordinates\n",
    "def load_image_and_crop(path, annotation_path):\n",
    "    \n",
    "    # * Decode the path to a string\n",
    "    path = path.numpy().decode('utf-8')\n",
    "    annotation_path = annotation_path.numpy().decode('utf-8')\n",
    "    \n",
    "    # * Get the bounding box coordinates\n",
    "    xmin, ymin, xmax, ymax = parse_xml(annotation_path)\n",
    "    \n",
    "    # * Load the image and crop it\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = image[ymin:ymax, xmin:xmax]\n",
    "    return image\n",
    "\n",
    "# - Preprocesses the image by resizing and normalizing it\n",
    "def preprocess_image(image):\n",
    "\n",
    "    # * Resize the image to be 224x224 pixels\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "\n",
    "    # * Normalize the image (Which means converting the pixel values to be between -0.5 and 0.5)\n",
    "    image = (image / 255.0) - 0.5 \n",
    "    return image\n",
    "\n",
    "# - Combines the above functions to load and preprocess the image\n",
    "def load_and_preprocess_image(path, annotation_path):\n",
    "\n",
    "    # * Get the cropped image\n",
    "    image = tf.py_function(func=load_image_and_crop, inp=[path, annotation_path], Tout=tf.uint8)\n",
    "    # * Set the shape of the image\n",
    "    image.set_shape([None, None, 3])\n",
    "\n",
    "    # * Preprocess the image\n",
    "    image = preprocess_image(image)\n",
    "\n",
    "    # * Return the image\n",
    "    return image\n",
    "\n",
    "# - Creates a map over the breed labels\n",
    "def create_breed_label(image_dir):\n",
    "    # * Get the breed labels\n",
    "    breed_labels = os.listdir(image_dir)\n",
    "\n",
    "    # * Create a dictionary to map the breed labels to integers\n",
    "    breed_label_map = {breed: idx for idx, breed in enumerate(breed_labels)}\n",
    "    return breed_label_map\n",
    "\n",
    "# - Prepares the dataset by loading and preprocessing the images\n",
    "def prepare_dataset(image_dir, annotation_dir, breed_label_map):\n",
    "    \n",
    "    # * Initialize lists to store the image and annotation paths\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # * Loop through the breed directories and extract the image and annotation paths\n",
    "    for breed_dir in os.listdir(image_dir):\n",
    "        breed_image_dir = os.path.join(image_dir, breed_dir) # ? Line to get the image directory for the breed.\n",
    "        breed_annotation_dir = os.path.join(annotation_dir, breed_dir) # ? Line to get the annotation directory for the breed.\n",
    "        breed_label = breed_label_map[breed_dir] # ? Add this line to get the breed.\n",
    "\n",
    "        # * Loop through the image files and get the image paths, annotation paths, and labels\n",
    "        for image_file in os.listdir(breed_image_dir):\n",
    "            \n",
    "            # Find the image path\n",
    "            image_path = os.path.join(breed_image_dir, image_file)\n",
    "            annotation_path = os.path.join(breed_annotation_dir, image_file.split('.')[0] + '.xml')\n",
    "\n",
    "            # Append the image path, annotation path, and label to the lists\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(breed_label)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int64)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: (load_and_preprocess_image(x, annotation_path), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# * Define the image and annotation directories for the dataset\n",
    "image_dir = 'data/images'\n",
    "annotation_dir = 'data/annotation-xml'\n",
    "\n",
    "# * Initialize the dataset\n",
    "dataset = prepare_dataset(image_dir, annotation_dir, create_breed_label(image_dir))\n",
    "\n",
    "# * Print the shape of the first 5 images\n",
    "for image, label in dataset.take(5):\n",
    "    print('Image shape:', image.shape, 'Label:', label.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Images and Annotations have been loaded into Python. So the next step is to split the dataset into three parts: *Train*, *Test* and *Validation*.\n",
    "\n",
    "We have chosen to do the typical ``70-20-10`` split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20580\n",
      "\n",
      "Number of samples in the training set: 14405 (70.00%)\n",
      "\n",
      "Number of samples in the validation set: 4117 (20.00%)\n",
      "\n",
      "Number of samples in the test set: 2058 (10.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def split_dataset(to_split, train_ratio=0.7, val_ratio=0.1, shuffle=True, seed=None, buffer_size=10000):\n",
    "    # ? Shuffle the dataset if shuffle is True\n",
    "    if shuffle:\n",
    "        # * Shuffle the dataset to randomize the order\n",
    "        to_split = to_split.shuffle(buffer_size=buffer_size, seed=seed, reshuffle_each_iteration=False)\n",
    "\n",
    "    # * Calculate the total number of samples\n",
    "    total_samples = tf.data.experimental.cardinality(to_split).numpy()\n",
    "\n",
    "    # - Calculate the number of samples for the training set and validation set\n",
    "    train_samples = int(total_samples * train_ratio)\n",
    "    val_samples = int(total_samples * val_ratio)\n",
    "\n",
    "    # * Split the dataset into training and test sets\n",
    "    train_dataset = to_split.take(train_samples)\n",
    "    remaining_dataset = to_split.skip(train_samples)\n",
    "\n",
    "    # * Split the test set into validation and test sets\n",
    "    val_dataset = remaining_dataset.take(val_samples)\n",
    "    test_dataset = remaining_dataset.skip(val_samples)\n",
    "\n",
    "    # * Return the datasets as train, validation, and test\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# - Split the dataset into training, validation, and testing datasets\n",
    "train_dataset, test_dataset, val_dataset = split_dataset(dataset)\n",
    "\n",
    "# * Output the dataset sizes and their approximate precentage of the total dataset\n",
    "total_samples = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_samples = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "val_samples = tf.data.experimental.cardinality(val_dataset).numpy()\n",
    "test_samples = tf.data.experimental.cardinality(test_dataset).numpy()\n",
    "\n",
    "print(f'Total samples: {total_samples}\\n')\n",
    "print('Number of samples in the training set:', train_samples, f'({train_samples / total_samples:.2%})\\n')\n",
    "print('Number of samples in the validation set:', val_samples, f'({val_samples / total_samples:.2%})\\n')\n",
    "print('Number of samples in the test set:', test_samples, f'({test_samples / total_samples:.2%})\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Compiling the model ğŸ”§\n",
    "\n",
    "The next step in the process is to compile the model itself. But before that we have define what **Loss function**, **Optimizer** and **Metrics** we are going to be using on this model.\n",
    "\n",
    "For the **Loss function** We have a few different options:\n",
    "\n",
    "(*Name a few different loss functions that would make sense to use for this project.*)\n",
    "\n",
    "For the **Optizimers** we also have a few different options:\n",
    "- *Adam*, *SGD*, *RMSProp* etc.\n",
    "\n",
    "For the **Metrcis** we also have a few different options:\n",
    "- *Accuarcy*, *PRecision*, *Recall*, *F1 score* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 322ms/step - accuracy: 0.0238 - loss: 4.5065 - val_accuracy: 0.0372 - val_loss: 4.5652\n",
      "Epoch 2/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 322ms/step - accuracy: 0.0371 - loss: 4.2888 - val_accuracy: 0.0406 - val_loss: 4.4942\n",
      "Epoch 3/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 315ms/step - accuracy: 0.0644 - loss: 4.0882 - val_accuracy: 0.0532 - val_loss: 4.4803\n",
      "Epoch 4/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 311ms/step - accuracy: 0.1272 - loss: 3.7139 - val_accuracy: 0.0466 - val_loss: 4.7841\n",
      "Epoch 5/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 337ms/step - accuracy: 0.3472 - loss: 2.6570 - val_accuracy: 0.0374 - val_loss: 6.2576\n",
      "Epoch 6/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 341ms/step - accuracy: 0.6126 - loss: 1.5494 - val_accuracy: 0.0296 - val_loss: 9.2603\n",
      "Epoch 7/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 362ms/step - accuracy: 0.7759 - loss: 0.8884 - val_accuracy: 0.0291 - val_loss: 12.1779\n",
      "Epoch 8/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 339ms/step - accuracy: 0.8580 - loss: 0.5901 - val_accuracy: 0.0282 - val_loss: 14.6902\n",
      "Epoch 9/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 338ms/step - accuracy: 0.9040 - loss: 0.4060 - val_accuracy: 0.0274 - val_loss: 14.8166\n",
      "Epoch 10/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 363ms/step - accuracy: 0.9325 - loss: 0.2770 - val_accuracy: 0.0308 - val_loss: 16.4567\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# ! Example of a simple CNN Model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(224, 224, 3)), # - Input layer with the shape of the images\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='relu'), # - Convolutional layer with 16 filters\n",
    "    tf.keras.layers.MaxPooling2D(), # - Max pooling layer\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'), # - Convolutional layer with 32 filters\n",
    "    tf.keras.layers.MaxPooling2D(), # - Max pooling layer\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'), # - Convolutional layer with 64 filters\n",
    "    tf.keras.layers.MaxPooling2D(), # - Max pooling layer\n",
    "    tf.keras.layers.Flatten(), # - Flatten the output of the convolutional layers\n",
    "    tf.keras.layers.Dense(512, activation='relu'), # - Dense layer with 512 units\n",
    "    tf.keras.layers.Dense(120, activation='softmax') # - Output layer with 120 units (One for each dog breed)\n",
    "])\n",
    "\n",
    "# * Compile the model (Maybe make more than one model and compare them to see which one is better?)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# * Train the model\n",
    "history = model.fit(\n",
    "    train_dataset.batch(32), # - Adjust the batch size as needed (32 is a common starting point)\n",
    "    validation_data=val_dataset.batch(32),\n",
    "    epochs=10 # - Adjust the number of epochs as needed.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Train the model ğŸ§ \n",
    "\n",
    "The next step in the process is to train the now compiled model on our data. Here we also have a little exploratory work in figuring out:\n",
    "- What *batch size* should we use?\n",
    "- What *number of epochs* should we use?\n",
    "- Is the model *overfitting* or *underfitting*?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher plan!\n",
    "\n",
    "1. **Choose the model architecture suitable for our problem** ğŸ¤”\n",
    "    - Convolutional Neural Network (CNN - Good with Image data)\n",
    "    - Recurrent Neural Network (RNN - Good with sequence data)\n",
    "    - Another type??\n",
    "\n",
    "2. **Compile our model** ğŸ”§\n",
    "    - What *Loss function* should we use? - Cross-entropy is used for classification?\n",
    "    - What *Optimizer* should we use? Adam, SGD, RMSProp etc.\n",
    "    - What *Metrics* should we use? Accuracy, precision, recall, f1 score etc.\n",
    "\n",
    "3. **Train the model** âš™ï¸\n",
    "    - What *batch size* should we use?\n",
    "    - What *number of epochs* should we use?\n",
    "    - Is the model *overfitting* or *underfitting*?\n",
    "\n",
    "4. **Evalute the model** ğŸ“Š\n",
    "    - Is the model performing as we would like? Based upon our selected metrics to be unbiased ğŸ˜‰\n",
    "\n",
    "5. **Tune Hyperparameter (Optional) - To improve performance** ğŸ“ˆ\n",
    "    - Use grid search or another thing similar to find the best hyperparameters\n",
    "    - Adjust model layers, units, learning rate etc.\n",
    "\n",
    "6. **Save the Model (Optional) - But would be smart** ğŸ§ \n",
    "    - This can be done, so we don't have to run all the code later to get the model up and running!\n",
    "\n",
    "7. **Use the Model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
